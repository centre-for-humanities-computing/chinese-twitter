{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content analysis\n",
    "## Set-up, clean-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "- Import data\n",
    "- Write cleaning function, apply to every df\n",
    "- Add circle.png as a mask for word cloud + use pyldavis layout&size to plot the topics together\n",
    "- Make word cloud topic model for Chinese\n",
    "- Prettify the frequency plot for most used tokens\n",
    "- Make a pretty frequency plot of language distribution in data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up matplotlib settings\n",
    "# set matplotlib aesthetics\n",
    "CB91_Blue = '#2CBDFE'\n",
    "CB91_Green = '#47DBCD'\n",
    "CB91_Pink = '#F3A0F2'\n",
    "CB91_Purple = '#9D2EC5'\n",
    "CB91_Violet = '#661D98'\n",
    "CB91_Amber = '#F5B14C'\n",
    "\n",
    "color_list = [CB91_Blue, CB91_Pink, CB91_Green, CB91_Amber,\n",
    "              CB91_Purple, CB91_Violet]\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=color_list)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.set(font='DIN Condensed Bold',\n",
    "        rc={\n",
    "            'axes.axisbelow': False,\n",
    "            'axes.edgecolor': 'lightgrey',\n",
    "            'axes.facecolor': 'None',\n",
    "            'axes.grid': False,\n",
    "            'axes.labelcolor': 'dimgrey',\n",
    "            'axes.spines.right': False,\n",
    "            'axes.spines.top': False,\n",
    "            'figure.facecolor': 'white',\n",
    "            'lines.solid_capstyle': 'round',\n",
    "            'patch.edgecolor': 'w',\n",
    "            'patch.force_edgecolor': True,\n",
    "            'text.color': 'dimgrey',\n",
    "            'xtick.bottom': False,\n",
    "            'xtick.color': 'dimgrey',\n",
    "            'xtick.direction': 'out',\n",
    "            'xtick.top': False,\n",
    "            'ytick.color': 'dimgrey',\n",
    "            'ytick.direction': 'out',\n",
    "            'ytick.left': False,\n",
    "            'ytick.right': False})\n",
    "\n",
    "sns.set_context(\"notebook\", rc={\"font.size\":16,\n",
    "                                \"axes.titlesize\":20,\n",
    "                                \"axes.labelsize\":18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning up\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "%pip install yellowbrick\n",
    "from yellowbrick.text import FreqDistVisualizer, TSNEVisualizer\n",
    "\n",
    "en_df_n = en_df.set_index('id_str')\n",
    "token_df = en_df_n.tokens.apply(pd.Series)\n",
    "en_df_n = pd.concat([en_df_n, token_df], axis=1)\n",
    "en_df_n.head()\n",
    "\n",
    "en_df_n.token[0].values()\n",
    "# Extract tokens as a list\n",
    "def tokens_to_list(row):\n",
    "    return row['token'].values()\n",
    "\n",
    "def lemmas_to_list(row):\n",
    "    return list(row['lemma'].values())\n",
    "\n",
    "def clean_tweet(row):\n",
    "    text = row['text']\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "en_df_n['token_new'] = en_df_n.apply(tokens_to_list, axis=1)\n",
    "en_df_n['lemma_new'] = en_df_n.apply(lemmas_to_list, axis = 1)\n",
    "en_df_n['text_clean'] = en_df_n.apply(clean_tweet, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisations\n",
    "### Frequent terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize frequent terms\n",
    "# clean text\n",
    "vect = CountVectorizer(stop_words='english', min_df=10, ngram_range=(1,2))\n",
    "docs = vect.fit_transform(en_df_n['text_clean'].dropna())\n",
    "features = vect.get_feature_names()\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "visualiser = FreqDistVisualizer(features=features)\n",
    "visualiser.fit(docs)\n",
    "\n",
    "visualiser.poof()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyLDAvis\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = ' '.join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join([ch for ch in stop_free if ch not in exclude])\n",
    "    normalized = ' '.join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "\n",
    "# some additional pre-processing is needed, so I'll use the original tweet-text column from the sample instead\n",
    "sample_text = en_df_n['text_clean']\n",
    "state_text = list(sample_text.values)\n",
    "\n",
    "text_clean = [clean(doc).split() for doc in state_text]\n",
    "dictionary = corpora.Dictionary(text_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in text_clean]\n",
    "\n",
    "# I'm keeping the number of topics small so that they would be easier to discern\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics = 8, id2word = dictionary, passes=100)\n",
    "\n",
    "tfidf = models.TfidfModel(doc_term_matrix)\n",
    "doc_tfidf = tfidf[doc_term_matrix]\n",
    "doc_lda = ldamodel[doc_tfidf]\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.gensim.prepare(ldamodel, doc_lda, dictionary, mds='tsne', sort_topics=True)\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "#pyLDAvis.save_html(panel, '8_topics_english.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda is assumed to be the variable holding the LdaModel object\n",
    "%pip install palettable\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from palettable.colorbrewer.qualitative import Dark2_8\n",
    "from palettable.lightbartlein.diverging import BlueDarkOrange18_5\n",
    "from palettable.matplotlib import Inferno_20\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n",
    "    return tuple(Inferno_20.colors[random.randint(1,15)])\n",
    "\n",
    "font_path = \"/Library/Fonts/DIN Condensed Bold.ttf\"\n",
    "icon_path = \"utils/circle.png\"\n",
    "\n",
    "mask = np.array(Image.open(icon_path))\n",
    "\n",
    "for t in range(ldamodel.num_topics):\n",
    "    plt.figure()\n",
    "    # wordcloud = WordCloud(background_color=\"white\", max_font_size=60, width=800, height=400)\n",
    "    wc = WordCloud(font_path=font_path, background_color=\"white\", max_words=2000, mask = mask,\n",
    "               stopwords=STOPWORDS,\n",
    "               max_font_size=100, random_state=42,\n",
    "                  width=800, height=400)\n",
    "    plt.imshow(wc.fit_words(dict(ldamodel.show_topic(t, 200))).recolor(color_func=color_func, random_state=3))\n",
    "    #wc.recolor(color_func=color_func)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Topic #\" + str(t+1))\n",
    "    plt.show()\n",
    "    #plt.savefig(f\"plots/english/en_wordcloud_topic_{t}.png\", dpi = 300)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drafts\n",
    "### Cluster analysis\n",
    "\n",
    "NB!! Change to clean text variables; Change HDBScan parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general prep\n",
    "real_clean = en_df['text']\n",
    "corpus_clean = list(real_clean.values)\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=10, ngram_range=(1,2))\n",
    "docs_clean = tfidf.fit_transform(corpus_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clusters = KMeans(n_clusters=10)\n",
    "clusters.fit(docs_clean)\n",
    "plt.figure(figsize=(16,10))\n",
    "tsne = TSNEVisualizer()\n",
    "tsne.fit(docs_clean, [\"c{}\".format(c) for c in clusters.labels_])\n",
    "tsne.poof()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBScan\n",
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5, gen_min_span_tree=True)\n",
    "clusterer.fit(docs_clean)\n",
    "plt.figure(figsize=(16,10))\n",
    "tsne = TSNEVisualizer()\n",
    "tsne.fit(docs_clean, [\"c{}\".format(c) for c in clusterer.labels_])\n",
    "tsne.poof()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('text_to_x': conda)",
   "language": "python",
   "name": "python361064bittexttoxconda878d44c121f744aca0a4b5eab155c3fe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
